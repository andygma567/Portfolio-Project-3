{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andygma567/Portfolio-Project-3/blob/main/My_Object_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "We use a TFRecord dataset of annotated images of people wearing facemasks taken from Roboflow. We use this dataset to train an EfficientDet model from the Tensorflow 2 Object Detection API via transfer learning. Please see the references at the end for additional information discussing how these models work. \n",
        "\n",
        "All of the work in this notebook follows from the set of tutorials referenced at the end. \n",
        "\n",
        "Finally after training a model on our custom dataset we use a second Colab notebook to visualize the inference of our trained model. \n",
        "\n",
        "## Notable Features\n",
        "\n",
        "Two notable features of this work are in the validation data evaluations and the image preprocessing that we add to the model configuration in the Section: 'Set up'. \n",
        "\n",
        "### Sequential valdiation dataset evaluation\n",
        "\n",
        "The TF2 OD API originally is written so that the validation data is evaluated in parallel with the training by running a script to do evaluation of the validation dataset with a separate processor. This approach uses more RAM and preprocessors than we have available. Therefore we modify the source code in the TF2 OD script for training models in order to run the validation dataset evaluation as a postprocess after the model training is done. Please see Subsection: 'Adjust the Core Code'. \n",
        "\n",
        "### A method for modifying model configs using the TF2 OD API utils\n",
        "\n",
        "In the Section: 'Set up' we add in additional image preprocessing steps. Adding in additional preprocessing steps to a model config file seems to be challenging to other users as well (please see Stack Overflow posts in Subsection: 'Change some of the image preprocessing') and the author is not aware of TF2 OD tutorials that explain how to adjust the model configs at the level of the config objects. In this work we present a method to create new preprocessing steps at the level of the config object using the utils from the TF2 OD API itself. This approach is different many of the approaches to modifying model configs by reading / writing text files with a custom script that are popular in online tutorials. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5QMOOEMtMF3O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Up"
      ],
      "metadata": {
        "id": "MEezQVBcJZnN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdEg7F8YmutP"
      },
      "source": [
        "## Download my Dataset\n",
        "\n",
        "My first goal is to find some data that I can train on.\n",
        "\n",
        "All training data needs to be in a TFRecord format to be used by  mdels in Tensorflow. TFRecord is a special kind of binary format that Google chooses to use because of it's speed for reading / writing and how space efficient it is. \n",
        "\n",
        "Normally, if I were to annontate my own image data set I would need to do the following:\n",
        "\n",
        "1.   Annontate my own images using a popular image annontation software such as LabelImg\n",
        "2.   Convert the file format (.XML ) to a TFRecord format using one of scripts in the TF2 Object Detection Repo utils folder (Please see Appendix)\n",
        "\n",
        "I have a file of images of people wearing masks already in my Google drive. I got this dataset from Roboflow (add link later) and it's already in a TFRecord format.\n",
        "\n",
        "I want to copy it and unzip it so that I can use it in my notebook. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJAmTb6viNIE"
      },
      "outputs": [],
      "source": [
        "# The ! in front is how we run command line arguments in a notebook. Alternatively we can also use %%bash\n",
        "# Because of the white space I gotta use quotes\n",
        "!cp \"/content/drive/MyDrive/Mask Wearing.v1-416x416-black-padding.tfrecord.zip\" .\n",
        "# If a file of the same name already exists then it will ask if you want to overwrite the file\n",
        "!unzip \"/content/Mask Wearing.v1-416x416-black-padding.tfrecord.zip\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8T5YCARkrsCu"
      },
      "source": [
        "## (Optional) A Comment about Label Maps\n",
        "\n",
        "I read online that tensorflow uses protobufs for efficiency and protobufs can be saved in 2 kinds of files: .pb (the unreadable binary form for large files) or .pbtxt (the readable text version)\n",
        "\n",
        "My image data needs lablel map too. This is a file that you can write by hand if you want (just save it as a .pbtxt). \n",
        "\n",
        "It seems like because I'm using a roboflow dataset the label map was already made for me. If I didn't have the labels in .pbtxt format there's the script that Tanner uses from the raccoon dataset that looks like it works for converting VOC XML to Tfrecord format. \n",
        "\n",
        "[Link to Tanner's tutorial blog](https://gilberttanner.com/blog/tensorflow-object-detection-with-tensorflow-2-creating-a-custom-model/)\n",
        "\n",
        "Also I read the [source code for label_map_util](https://github.com/tensorflow/models/blob/master/research/object_detection/utils/label_map_util.py)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# My labels map is a .pbtxt file that looks like this\n",
        "\n",
        "# It's important that the id starts at index 1 because index = 0 is reserved for 'no detection' \n",
        "'''\n",
        "item {\n",
        "    name: \"mask\",\n",
        "    id: 1,\n",
        "    display_name: \"mask\"\n",
        "}\n",
        "item {\n",
        "    name: \"no-mask\",\n",
        "    id: 2,\n",
        "    display_name: \"no-mask\"\n",
        "}\n",
        "'''"
      ],
      "metadata": {
        "id": "iOIh9p5rNyNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANiDXb5bWMFU"
      },
      "source": [
        "## Installing TF2 Object Detection API \n",
        "\n",
        "Three of the important libraries that we use in this notebook are CUDA (the code that manages the GPU usage), Tensorflow, and the TF2 OD repo. Google Colab comes preinstalled with these CUDA and Tensorflow but it's important to check that the installed versions of these libraries are compatbile. \n",
        "\n",
        "I ran into [this compatability error](https://stackoverflow.com/questions/71000120/colab-0-unimplemented-dnn-library-is-not-found) when I first ran this notebook. \n",
        "\n",
        "I think updating CUDA to the latest version fixed my issue. Sometimes you need to do some trial and error work with different installations until you find a set that works for you. I'm using tf v 2.9, libcudnn8 8.5.0.96-1+cuda11.7, and the TF OD repo in August 2022. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VLeKYpnDRDd0",
        "outputId": "93fb41b4-09c8-4489-8662-a13dbfd7576a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.9.1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "No-HcVe9xdD2"
      },
      "outputs": [],
      "source": [
        "# Check libcudnn8 version - it's pretty up to date 8.0.5.39-1+cuda11.1\n",
        "!apt-cache policy libcudnn8\n",
        "# This installs the a more uptodate version of CUDA\n",
        "!apt install --allow-change-held-packages libcudnn8=8.5.0.96-1+cuda11.7"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the Tensorflow Object Detection Repo\n",
        "\n",
        "Tensorflow keeps a repo on Github called Model Garden (Insert link later). Model Garden contains all of their state of the art models and in the research folder is there collection of state of the art object detection models that we will use. \n"
      ],
      "metadata": {
        "id": "f1oDhFBCH5HQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypWGYdPlLRUN",
        "outputId": "254a7d0e-1673-4c19-d1af-d95bb2ee3bc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'models'...\n",
            "remote: Enumerating objects: 3467, done.\u001b[K\n",
            "remote: Counting objects: 100% (3467/3467), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2904/2904), done.\u001b[K\n",
            "remote: Total 3467 (delta 898), reused 1488 (delta 506), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (3467/3467), 46.88 MiB | 21.53 MiB/s, done.\n",
            "Resolving deltas: 100% (898/898), done.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "# Clone the tensorflow models repository if it doesn't already exist\n",
        "if \"models\" in pathlib.Path.cwd().parts:\n",
        "  while \"models\" in pathlib.Path.cwd().parts:\n",
        "    os.chdir('..')\n",
        "elif not pathlib.Path('models').exists():\n",
        "  !git clone --depth 1 https://github.com/tensorflow/models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adjust the Core Code\n",
        "\n",
        "I modify the source code as described in this blog post:\n",
        "https://thachngoctran.medium.com/make-tensorflows-object-detection-validation-a-true-post-process-c45785f08d3a\n",
        "\n",
        "This will make evaluating the model on my validation data a post process that I can run right after the training instead of running the evaluation on validation data in paralell. References on how to run a script in parallel are listed in the appendices at the end. \n",
        "\n",
        "I changed the file /content/models/official/object_detection/model_lib_v2.py in the source code and saved it as seq_eval_model_lib_v2.py in my Google Drive. \n",
        "\n",
        "The following code gets my changed python code and uses it to overwrite the file model_lib_v2.py that originally came from the Github repo download. \n",
        "\n",
        "It is important that this step is done before compiling the protos and installing the Github repo. "
      ],
      "metadata": {
        "id": "bday0XhMIrhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get my modifed script for sequential evaluation of the validation data\n",
        "# and replace the existing script that is written for parallel validation eval\n",
        "!cp \"/content/drive/MyDrive/Object Detection/seq_eval_model_lib_v2.py\" .\n",
        "!mv \"seq_eval_model_lib_v2.py\" \"model_lib_v2.py\"\n",
        "!mv \"model_lib_v2.py\" \"/content/models/research/object_detection/model_lib_v2.py\""
      ],
      "metadata": {
        "id": "6BTF4uc5Ent3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The changes made to model_lib_v2.py\n",
        "\n",
        "Change 1)\n",
        "\n",
        "In module \"model_lib_v2.py\", function train_loop(), line 452: Replace\n",
        "```\n",
        "...\n",
        "checkpoint_max_to_keep=7,\n",
        "...\n",
        "```\n",
        "with \n",
        "```\n",
        "checkpoint_max_to_keep=A_LARGE_NUM,\n",
        "```\n",
        "I set the parameter max_checkpoint_to_keep to be None. According to this API and this source code setting it to be None means it will save every checkpoint: \n",
        "\n",
        "*   [source code on TF OD github](https://github.com/tensorflow/models/blob/master/research/object_detection/model_lib_v2.py)\n",
        "*   [TF Checkpoint Manager API](https://www.tensorflow.org/api_docs/python/tf/train/CheckpointManager)\n",
        "\n",
        "Change 2)\n",
        "\n",
        "See model_lib_v2.py, function eval_continuously(), line 1136: Replace\n",
        "\n",
        "```\n",
        "for latest_checkpoint in tf.train.checkpoints_iterator(\n",
        "    checkpoint_dir, timeout=timeout, min_interval_secs=wait_interval):\n",
        "  ...\n",
        "ckpt.restore(latest_checkpoint).expect_partial()\n",
        "```\n",
        "with \n",
        "\n",
        "```\n",
        "import regex as re\n",
        "import glob\n",
        "def natural_sort(l): \n",
        "  convert = lambda text: int(text) if text.isdigit() else text.lower() \n",
        "  alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ] \n",
        "  return sorted(l, key = alphanum_key)\n",
        "for latest_checkpoint in natural_sort(list(set(map(lambda n: n[:n.index(\".\")], glob.glob(f\"{checkpoint_dir}/ckpt-*.*\"))))):\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "t4TndpBGlcbj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install the Object Detection Repo\n",
        "\n",
        "This code installs all of the protos using a proto compiler. The compiler reads all of the .proto files and then saves new .py scripts in the current directory (as specified by the --python_out=. argument)"
      ],
      "metadata": {
        "id": "O8Av3qr5f2Zs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QPmVBSlLTzM"
      },
      "outputs": [],
      "source": [
        "# Install the Object Detection API\n",
        "%%bash\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "cp object_detection/packages/tf2/setup.py .\n",
        "python -m pip install ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHfsJ5nWLWh9"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import random\n",
        "import io\n",
        "import imageio\n",
        "import glob\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "from six import \n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from IPython.display import display, Javascript\n",
        "from IPython.display import Image as IPyImage\n",
        "\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import config_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from object_detection.utils import colab_utils\n",
        "from object_detection.builders import model_builder\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1rSOqNJogFx"
      },
      "source": [
        "## I need to download the models and config files\n",
        "\n",
        "By convention every blog tutorial says to download a model checkpoint as a tar.gz file with !wget\n",
        "\n",
        "I think the tar.gz file is just the training checkpoint for loading the pretrained weights. I got the link by going to the TF2 model zoo and hovering my mouse over the model and then copying the link address by right clicking. I really don't know if there is another way to do this\n",
        "\n",
        "Here is a link to the [TF2 OD Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDK80c8tkI6z",
        "outputId": "cc4ea254-2112-412e-f113-94115823f02c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/my_model_dir\n"
          ]
        }
      ],
      "source": [
        "%mkdir /content/models/my_model_dir/\n",
        "%cd /content/models/my_model_dir/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVpl_o7xpjkX",
        "outputId": "c548b075-1d49-4955-a6d3-52d542348861"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-26 08:34:38--  http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d0_coco17_tpu-32.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 142.251.5.128, 2a00:1450:400c:c1b::80\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|142.251.5.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 30736482 (29M) [application/x-tar]\n",
            "Saving to: ‘efficientdet_d0_coco17_tpu-32.tar.gz’\n",
            "\n",
            "efficientdet_d0_coc 100%[===================>]  29.31M  32.4MB/s    in 0.9s    \n",
            "\n",
            "2022-08-26 08:34:40 (32.4 MB/s) - ‘efficientdet_d0_coco17_tpu-32.tar.gz’ saved [30736482/30736482]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# I'm getting the efficientdet_d0 maybe next time it would be better to use efficientdet_d4\n",
        "!wget {\"http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d0_coco17_tpu-32.tar.gz\"}\n",
        "\n",
        "# This tar command extracts the file\n",
        "# I think -x tells them that we're extracting as opposed to -c for compressing\n",
        "# and -f is to let the script know that the next argument is the name of the file\n",
        "!tar -xf efficientdet_d0_coco17_tpu-32.tar.gz\n",
        "\n",
        "# This extracted directory also comes with the config file and the checkpoints in it"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write a config file"
      ],
      "metadata": {
        "id": "RC1ZXyVXJYwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# These are the paths to the data files\n",
        "BASE_CONFIG_PATH = 'efficientdet_d0_coco17_tpu-32/pipeline.config'\n",
        "TRAIN_RECORD_PATH = '/content/train/People.tfrecord'\n",
        "VAL_RECORD_PATH = '/content/valid/People.tfrecord'\n",
        "# When you load checkpoints you give it the prefix of the two files .data and .index\n",
        "FINE_TUNE_CKPT_PATH = 'efficientdet_d0_coco17_tpu-32/checkpoint/ckpt-0'\n",
        "LABEL_MAP_PATH = '/content/train/People_label_map.pbtxt'\n",
        "SAVE_MY_CONFIG_PATH = '/content/models/my_model_dir'\n",
        "\n",
        "# Read the configuration file into a dictionary of config objects\n",
        "configs = config_util.get_configs_from_pipeline_file(BASE_CONFIG_PATH)\n",
        "label_map = label_map_util.get_label_map_dict(LABEL_MAP_PATH)\n",
        "NUM_CLASSES = len(label_map.keys())\n",
        "# I think .WhichOneOf() is for a config object and it returns a str that's describes the instance of the config obj\n",
        "meta_arch = configs[\"model\"].WhichOneof(\"model\") # The model architecture\n",
        "\n",
        "NUM_CLASSES = 2 # The mask dataset has 2 classes\n",
        "BATCH_SIZE = 16 # A lot of tutorials were saying batch_size 32 uses too much RAM\n",
        "NUM_STEPS = 400 # It takes about 1 sec/ step and 1000 steps takes roughly 16min\n",
        "CKPT_EVERY_N = 200 \n",
        "\n",
        "# Some tutorials use a parameter called num_eval_steps to do an evaluation after \n",
        "# a specified number of training steps but I believe that this is deprecated in TF2"
      ],
      "metadata": {
        "id": "6ynxu_P0qpfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Optional) Change some of the Image Preprocessing\n",
        "\n",
        "We will now add some additional image preprocessing. When we compile the protos the compiler will create new python files. We will use the newly created file *preprocessor_pb2.py* to create new preprocessing steps. \n",
        "\n",
        "A full list of what preprocessing options are available can be found in either the *preprocessor_pb2.py* file or in the [preprocessor.proto file in the TF2 OD repo](https://github.com/tensorflow/models/blob/master/research/object_detection/protos/preprocessor.proto)\n",
        "\n",
        "I describe how to add new preprocessing configurations in more detail in my answer to these posts on Stack Overflow: \n",
        "* [Dynamically Editing Pipeline Config...](https://stackoverflow.com/questions/55323907/dynamically-editing-pipeline-config-for-tensorflow-object-detection)\n",
        "* [How to change data augmentation parameters dynamically...](https://stackoverflow.com/questions/65768729/how-to-change-data-augmentation-parameters-dynamically-in-the-config-file-of-ten)"
      ],
      "metadata": {
        "id": "n23a0Eu5vnxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from object_detection.protos import preprocessor_pb2\n",
        "\n",
        "# Construct a new PreprocessingStep() object\n",
        "my_data_aug_1 = preprocessor_pb2.PreprocessingStep()\n",
        "# Set the PreprocessingStep object's field\n",
        "# random_rgb_to_gray has only one subfiell 'probability' that can be set\n",
        "my_data_aug_1.random_rgb_to_gray.probability = 0.1\n",
        "\n",
        "# I don't think it's possible to set a PreprocessingStep() to have multiple \n",
        "# preprocessing fields. Things just get overwritten\n",
        "\n",
        "my_data_aug_2 = preprocessor_pb2.PreprocessingStep()\n",
        "my_data_aug_2.random_adjust_brightness.max_delta = 0.2\n",
        "\n",
        "my_data_aug_3 = preprocessor_pb2.PreprocessingStep()\n",
        "my_data_aug_3.random_adjust_contrast.min_delta = 0.8\n",
        "my_data_aug_3.random_adjust_contrast.max_delta = 1.25\n",
        "\n",
        "my_data_aug_4 = preprocessor_pb2.PreprocessingStep()\n",
        "my_data_aug_4.random_adjust_hue.max_delta = 0.02\n",
        "\n",
        "# configs['train_config'].data_augmentation_options is a list of PreprocessingStep objects \n",
        "configs['train_config'].data_augmentation_options.append(my_data_aug_1)\n",
        "configs['train_config'].data_augmentation_options.append(my_data_aug_2)\n",
        "configs['train_config'].data_augmentation_options.append(my_data_aug_3)\n",
        "configs['train_config'].data_augmentation_options.append(my_data_aug_4)\n",
        "\n",
        "print(configs['train_config'].data_augmentation_options)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ek6-M8r1vmWA",
        "outputId": "b9142f3a-90d7-4b45-df7d-a54be6a55620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[random_horizontal_flip {\n",
            "}\n",
            ", random_scale_crop_and_pad_to_square {\n",
            "  output_size: 512\n",
            "  scale_min: 0.10000000149011612\n",
            "  scale_max: 2.0\n",
            "}\n",
            ", random_rgb_to_gray {\n",
            "  probability: 0.10000000149011612\n",
            "}\n",
            ", random_adjust_brightness {\n",
            "  max_delta: 0.20000000298023224\n",
            "}\n",
            ", random_adjust_contrast {\n",
            "  min_delta: 0.800000011920929\n",
            "  max_delta: 1.25\n",
            "}\n",
            ", random_adjust_hue {\n",
            "  max_delta: 0.019999999552965164\n",
            "}\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the utils in the repo to build a config file using the steps explained in here: https://github.com/yasserius/tf2-object-detection-api\n",
        "\n"
      ],
      "metadata": {
        "id": "7GWuArdEyVdl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IWe8IUZqpfM"
      },
      "outputs": [],
      "source": [
        "# I think this is for updating / overwriting existing stuff in the config file\n",
        "override_dict = {\n",
        "  'model.{}.num_classes'.format(meta_arch): NUM_CLASSES,\n",
        "  'train_config.batch_size': BATCH_SIZE,\n",
        "  'train_input_path': TRAIN_RECORD_PATH,\n",
        "  'eval_input_path': VAL_RECORD_PATH,\n",
        "  'train_config.fine_tune_checkpoint': FINE_TUNE_CKPT_PATH,\n",
        "  'label_map_path': LABEL_MAP_PATH, \n",
        "  'train_config.use_bfloat16': False, # Set this to True only if you use TPUs\n",
        "  'train_config.num_steps': NUM_STEPS,\n",
        "  'train_config.fine_tune_checkpoint_type': \"detection\", # b/c we are doing detection not classification\n",
        "}\n",
        "# Alternatively we could have modified the subfields of elements in the configs dictionary\n",
        "# e.g. configs['train_config'].train_input_path = TRAIN_RECORD_PATH (I think)\n",
        "\n",
        "# The Hparams arg is deprecated in TF2 so we use kwargs_dict to override the existing configs\n",
        "configs = config_util.merge_external_params_with_configs(configs, kwargs_dict=override_dict) \n",
        "# Make a new pipeline config proto\n",
        "pipeline_config = config_util.create_pipeline_proto_from_configs(configs)\n",
        "# This method can only save a config as 'pipeline.config'\n",
        "config_util.save_pipeline_config(pipeline_config, SAVE_MY_CONFIG_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv pipeline.config my_pipeline.config\n",
        "%cat my_pipeline.config "
      ],
      "metadata": {
        "id": "DBuF_yqsyro7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vkz-ozhxoXm"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9MlKr5b0Dvi"
      },
      "source": [
        "We can use Tensorboard to monitor the training of our model. \n",
        "\n",
        "First we use enable our internet browser to allow certain third party cookies so that we can see the Tensorboard in the Colab notebook. In my case because I use Google Chrome's browser I changed the privacy settings to allow 3rd party cookies from [*.]colab.research.google.com\n",
        "\n",
        "For more information on how to enable cookies for just colab please see this Stack Overflow post [Allow a Google Colab domain cookies on chrome](https://stackoverflow.com/questions/50289535/allow-a-google-colab-domain-cookies-on-chrome)\n",
        "\n",
        "In order to monitor training with Tensorboard it is important to run a cell to open Tensorboard before running the cell with the training script. We can refresh the Tensorboard while the training script is running in order to monitor our progress. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qylQUL5y5MA"
      },
      "outputs": [],
      "source": [
        "# Make a training directory that will hold all of our training and validation evaluation \n",
        "# TFevent files and our checkpoint files from training\n",
        "!mkdir training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWbHIdkvmE1h"
      },
      "outputs": [],
      "source": [
        "# Set the file path to our training directory \n",
        "MODEL_DIR = 'training'\n",
        "# Set the file path to our previously made pipeline config file\n",
        "MY_CONFIG_PATH = '/content/models/my_model_dir/my_pipeline.config'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91V2lGm9y5MA"
      },
      "outputs": [],
      "source": [
        "# I need to keep clicking refresh as it continues to train in order to see the graphs in tensorboard\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir 'training'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "I5kGumpay5MB"
      },
      "outputs": [],
      "source": [
        "# The parameter --alsologtostderr is to save the logs / checkpoints into the model_dir directory\n",
        "!python /content/models/research/object_detection/model_main_tf2.py \\\n",
        "    --pipeline_config_path={MY_CONFIG_PATH} \\\n",
        "    --model_dir={MODEL_DIR} \\\n",
        "    --alsologtostderr \\\n",
        "    --checkpoint_every_n={CKPT_EVERY_N} \\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2gGynuQciZF"
      },
      "source": [
        "# Run Evaluation on Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cm8uPbbZ4Iqo"
      },
      "outputs": [],
      "source": [
        "# The parameter sample_1_of_n_eval_examples=1 takes every single image from an eval batch for evaluation\n",
        "# no image is skipped\n",
        "!python  /content/models/research/object_detection/model_main_tf2.py \\\n",
        "    --alsologtostderr \\\n",
        "    --pipeline_config_path=\"my_pipeline.config\" \\\n",
        "    --model_dir=\"training\" \\\n",
        "    --sample_1_of_n_eval_examples=1 \\\n",
        "    --checkpoint_dir=\"training\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the Model on the Test Data\n",
        "\n",
        "We use the Tensorboard to hand-select a checkpoint to restor our model from and then we run the model on the test data set. We will rewrite the file 'checkpoint' in the training directory because the models run based on the latest checkpoint listed in this text file. \n",
        "\n",
        "We create a new directory for testing and copy the relevant checkpoints to that directory for the *model_main_tf2.py* script"
      ],
      "metadata": {
        "id": "Wz6B9qd-38W3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose my checkpoint for testing the model from \n",
        "MY_CKPT = \"ckpt-17\""
      ],
      "metadata": {
        "id": "rorAew-1MeDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rewrite the checkpoint file according to my choosen checkpoint\n",
        "# This code rewrites the latest checkpoint in the 'checkpoint' file to be MY_CKPT\n",
        "with open(os.path.join(MODEL_DIR, 'checkpoint'), mode = 'r') as ckpt_file:\n",
        "  lines = ckpt_file.readlines()\n",
        "  first_line = lines[0].split('\\\"')\n",
        "  first_line[1] = MY_CKPT\n",
        "  lines[0] = '\\\"'.join(first_line)\n",
        "with open(os.path.join(MODEL_DIR, 'checkpoint'), mode = 'w') as ckpt_file:\n",
        "  ckpt_file.write(''.join(lines))"
      ],
      "metadata": {
        "id": "rhr2qlO57N5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a directory and copy over all of the checkpoint files that I need to run an evaluation\n",
        "import glob\n",
        "\n",
        "!rm -r testing\n",
        "!mkdir \"testing\"\n",
        "\n",
        "for file_name in glob.glob(os.path.join(MODEL_DIR, MY_CKPT) + '*'):\n",
        "  print(file_name)\n",
        "  !cp {file_name} 'testing'\n",
        "!cp '/content/models/my_model_dir/training/checkpoint' 'testing'"
      ],
      "metadata": {
        "id": "BlXec6gw8-_b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c4a2914-342c-47a0-bcbd-c2cec52263b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training/ckpt-17.index\n",
            "training/ckpt-17.data-00000-of-00001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a custom config file using the test data as the eval input\n",
        "import re\n",
        "from google.protobuf import text_format\n",
        "\n",
        "from object_detection.utils import config_util\n",
        "from object_detection.utils import label_map_util\n",
        "\n",
        "TEST_RECORD_PATH = '/content/test/People.tfrecord'\n",
        "SAVE_TEST_CONFIG_PATH = '/content/models/my_model_dir/testing'\n",
        "\n",
        "# We make a new config file for the testing\n",
        "configs = config_util.get_configs_from_pipeline_file(MY_CONFIG_PATH)\n",
        "override_dict = {\n",
        "  'eval_input_path': TEST_RECORD_PATH\n",
        "}\n",
        "configs = config_util.merge_external_params_with_configs(configs, kwargs_dict=override_dict) \n",
        "pipeline_config = config_util.create_pipeline_proto_from_configs(configs)\n",
        "config_util.save_pipeline_config(pipeline_config, SAVE_TEST_CONFIG_PATH)"
      ],
      "metadata": {
        "id": "-6t_PdbERnBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv testing/pipeline.config testing/test_pipeline.config\n",
        "%cat testing/test_pipeline.config "
      ],
      "metadata": {
        "id": "ac1dluECRnBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open a new Tensorboard to monitor the results of our model on the testing data\n",
        "%tensorboard --logdir 'testing'"
      ],
      "metadata": {
        "id": "rqw_ZF1oXJhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python  /content/models/research/object_detection/model_main_tf2.py \\\n",
        "    --alsologtostderr \\\n",
        "    --pipeline_config_path=\"/content/models/my_model_dir/testing/test_pipeline.config\" \\\n",
        "    --model_dir=\"testing\" \\\n",
        "    --sample_1_of_n_eval_examples=1 \\\n",
        "    --checkpoint_dir=\"testing\""
      ],
      "metadata": {
        "id": "C9x8l2X5XQW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-M48hGPM9d0r"
      },
      "source": [
        "# Save and Export the Model\n",
        "\n",
        "We save the model using the *exporter_main_v2.py* script. This script will save a 'SavedModel' directory in another directory it creates from the \n",
        "'output_directory' parameter. \n",
        "\n",
        "Then we copy this SavedModel directory to my Google Drive. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_XIp_Zb9d0s"
      },
      "outputs": [],
      "source": [
        "OUTPUT_DIR = 'trained_model'\n",
        "LAST_MODEL_PATH = 'training'\n",
        "\n",
        "!python /content/models/research/object_detection/exporter_main_v2.py \\\n",
        "    --input_type=image_tensor \\\n",
        "    --trained_checkpoint_dir={LAST_MODEL_PATH} \\\n",
        "    --output_directory={OUTPUT_DIR} \\\n",
        "    --pipeline_config_path={MY_CONFIG_PATH}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model to my Google Drive and we'll test the model in another Colab\n",
        "# The flag -r is for 'recursive' in order to copy the entire directory over \n",
        "!cp -r /content/models/my_model_dir/trained_model /content/drive/MyDrive"
      ],
      "metadata": {
        "id": "qztyrGDkdIyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusions / Further Work\n",
        "\n",
        "In another Colab we will test the inference abilities of our trained model. "
      ],
      "metadata": {
        "id": "BcDAGtEBka_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Appendix A (Running Validation eval in Parallel)\n",
        "\n",
        "Originally the validation is code is meant to be executing the same model_main_tf2.py script (while passing in an arugment for checkpoint_directory) in another terminal window while the first execution of model_main_tf2.py is still running. The evaluation is done by waiting for new checkpoints / tf events to be produced by the first (training) script. I think the default waiting time can be changed by passing in an argument to the model_main_tf2.py (for evaluation) script call. \n",
        "\n",
        "When I try to run the training and evaluation in parallel (like it was designed) it crashes because of I run out of RAM. I think I would need about 16GB of RAM and at least two processors to do this. \n",
        "\n",
        "People suggest running a parallel script by opening another terminal (and even allocating a processor e.g. CPU if needed). Please see these posts for reference of parallel processing:\n",
        "* https://stackoverflow.com/questions/64510791/tf2-object-detection-api-model-main-tf2-py-validation-loss\n",
        "* https://github.com/yasserius/tf2-object-detection-api\n",
        "* https://techzizou.com/training-an-ssd-model-for-a-custom-object-using-tensorflow-2-x/#tf2_step14\n",
        "\n"
      ],
      "metadata": {
        "id": "Km_lkHpXjrUH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Appendix B (Converting Pascal VOC .xml format to TFRecord format)\n",
        "\n",
        "Here are a couple of links related how to convert files to TFRecord format using the utils in the TF2 OD repo. \n",
        "\n",
        "*   [TF2 OD Dataset_tools](https://github.com/tensorflow/models/tree/master/research/object_detection/dataset_tools)\n",
        "*   [Convert Pascal VOC dataset into TFRecord format in 12 Steps.](https://medium.com/@shwetaka1988/convert-pascal-voc-dataset-into-tfrecord-format-in-12-steps-eedbfa37dac1)\n",
        "*   [Understanding TFRecord format](https://www.kaggle.com/code/gauravchopracg/understanding-tfrecord-format/notebook)\n",
        "*   [Keras Tutorial - Creating TFRecords](https://keras.io/examples/keras_recipes/creating_tfrecords/#generate-data-in-the-tfrecord-format)\n",
        "*   [Roboflow App method to create TFRecords](https://blog.roboflow.com/create-tfrecord/)"
      ],
      "metadata": {
        "id": "8hAF4DVqmiML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Appendix C (Alternative ways to do Object Detection)\n",
        "\n",
        "At the time of writing this notebook (September 2022) I found at least two other popular ways to train custom object detection models. \n",
        "\n",
        "## Option 1) Tensorflow Lite model maker \n",
        "This is an experimental module in Tensorflow Lite. This approach can create and train a tensorflow lite model on a custom dataset in just few lines of code. The notation is very similar to what is used to train cars models making it very easy to pick up. However, the model options are very limited to just one kind of object detection model and there's much less customization allowed. For example, you can't have new image pre-processing to your model. \n",
        "\n",
        "\n",
        "\n",
        "*   [12 min youtube tutorial](https://www.youtube.com/watch?v=-ZyFYniGUsw)\n",
        "> (Also checkout the colab notebook in the video description)\n",
        "*   [Object Detection with TensorFlow Lite Model Maker](https://www.tensorflow.org/lite/models/modify/model_maker/object_detection)\n",
        "\n",
        "\n",
        "## Option 2) Detectron2\n",
        "\n",
        "This is another popular object detection repo built and maintained by Meta (formerly facebook). It offers similar services to the TF2 Object Dectection API but it is build for PyTorch. It is notable that Detectron2 offers a many more models for creating masks and doing image segmentation than the TF2 OD API.\n",
        "* [Detectron2 Model Zoo ](https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md)"
      ],
      "metadata": {
        "id": "8M3zL9A4_EXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References \n",
        "\n",
        "**Tutorials**\n",
        "\n",
        "1.   https://neptune.ai/blog/how-to-train-your-own-object-detector-using-tensorflow-object-detection-api\n",
        "2.   https://neptune.ai/blog/tensorflow-object-detection-api-best-practices-to-training-evaluation-deployment\n",
        "3.   https://gilberttanner.com/blog/tensorflow-object-detection-with-tensorflow-2-creating-a-custom-model/\n",
        "4.   https://github.com/yasserius/tf2-object-detection-api\n",
        "5.   https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/auto_examples/plot_object_detection_saved_model.html\n",
        "6.   https://blog.roboflow.com/train-a-tensorflow2-object-detection-model/\n",
        "7.   [Roboflow Public Datasets](https://public.roboflow.com/)\n",
        "8.   https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tf2_colab.ipynb\n",
        "\n",
        "**EfficientNet**\n",
        "\n",
        "1.   [EfficientNet: Improving Accuracy and Efficiency through AutoML and Model Scaling](https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html)\n",
        "2.   [A Medium Blogpost on EfficientNet](https://medium.com/@nainaakash012/efficientnet-rethinking-model-scaling-for-convolutional-neural-networks-92941c5bfb95)\n",
        "\n",
        "**EfficientDet**\n",
        "\n",
        "1.   [A nice series of blog posts on EfficientDet](https://amaarora.github.io/2021/01/11/efficientdet.html#bifpn-bi-directional-feature-pyramid-network)\n",
        "2.   [Arictle: EfficientDet: Guide to State of The Art Object Detection Model\n",
        "](https://analyticsindiamag.com/efficientdet/)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AzdNRnnnMKB4"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1-lmHhwBddEIOmowmVByHk2jkbSExo-b4",
      "authorship_tag": "ABX9TyMtkVJGwns7awkf4W516bew",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}